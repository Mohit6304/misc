{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# WEEK-1\n",
        "##Tokenization using UNIX commands"
      ],
      "metadata": {
        "id": "59nH5o6pmumW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "### 1. **Create a text file `text1.txt` containing text of your choice.**\n",
        "**Answer:**\n",
        "```bash\n",
        "echo \"This is an example text file.\" > text1.txt\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Use `tr` utility to replace every occurrence of a given character with another given character.**\n",
        "**Answer:**\n",
        "```bash\n",
        "tr 'o' 'x' < example.txt\n",
        "```\n",
        "**Output:**\n",
        "```plaintext\n",
        "Hellx Wxrld!\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **What are `-s` and `-c` options of `tr` command?**\n",
        "**Answer:**\n",
        "- `-s` option (squeeze-repeats): This option is used to squeeze (reduce) repeated characters in the input to a single character.\n",
        "- `-c` option (complement): This option is used to complement the set of characters. When used, it replaces characters not listed in the first set with the corresponding character from the second set.\n",
        "\n",
        "**Examples:**\n",
        "\n",
        "```bash\n",
        "echo \"aaabbbccc\" | tr -s 'a'\n",
        "```\n",
        "**Output:**\n",
        "```plaintext\n",
        "abbbc\n",
        "```\n",
        "\n",
        "```bash\n",
        "echo \"abc123\" | tr -c 'a-z' 'X'\n",
        "```\n",
        "**Output:**\n",
        "```plaintext\n",
        "XXX123\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **What is the output of the following command?**\n",
        "```bash\n",
        "tr a-z A-Z < text1.txt\n",
        "```\n",
        "**Answer:**\n",
        "The command `tr a-z A-Z < text1.txt` will transform all lowercase letters to uppercase in the content of the file `text1.txt`. The output will be displayed on the terminal.\n",
        "\n",
        "**Example output:**\n",
        "```plaintext\n",
        "THIS IS AN EXAMPLE TEXT FILE.\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **Create a text file `text2.txt` containing any text (one word per line) and sort the words in the `text2.txt` file using the `sort` command.**\n",
        "**Answer:**\n",
        "```bash\n",
        "echo -e \"banana\\napple\\norange\\nkiwi\\ngrape\" > text2.txt\n",
        "sort text2.txt\n",
        "```\n",
        "**Output:**\n",
        "```plaintext\n",
        "apple\n",
        "banana\n",
        "grape\n",
        "kiwi\n",
        "orange\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 6. **Sort and display unique lines in the `text2.txt` file.**\n",
        "**Answer:**\n",
        "```bash\n",
        "sort -u text2.txt\n",
        "```\n",
        "**Output:**\n",
        "```plaintext\n",
        "apple\n",
        "banana\n",
        "grape\n",
        "kiwi\n",
        "orange\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 7. **Sort and display unique lines in `text2.txt` such that each word is preceded with the frequency count of the word in the `text2.txt` file.**\n",
        "**Answer:**\n",
        "```bash\n",
        "sort text2.txt | uniq -c\n",
        "```\n",
        "**Output:**\n",
        "```plaintext\n",
        "      1 apple\n",
        "      1 banana\n",
        "      1 grape\n",
        "      1 kiwi\n",
        "      1 orange\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 8. **Obtain and display the tokens in `text1.txt` file.**\n",
        "**Answer:**\n",
        "```bash\n",
        "tr -sc 'A-Za-z' '\\n' < text1.txt | grep -v '^$'\n",
        "```\n",
        "**Input (in `text1.txt`):**\n",
        "```plaintext\n",
        "This is an example text file.\n",
        "```\n",
        "\n",
        "**Output:**\n",
        "```plaintext\n",
        "This\n",
        "is\n",
        "an\n",
        "example\n",
        "text\n",
        "file\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 9. **Display the tokens in sorted order.**\n",
        "**Answer:**\n",
        "```bash\n",
        "tr -sc 'A-Za-z' '\\n' < text1.txt | grep -v '^$' | sort\n",
        "```\n",
        "**Output:**\n",
        "```plaintext\n",
        "This\n",
        "an\n",
        "example\n",
        "file\n",
        "is\n",
        "text\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 10. **Display the unique tokens in sorted order.**\n",
        "**Answer:**\n",
        "```bash\n",
        "tr -sc 'A-Za-z' '\\n' < text1.txt | grep -v '^$' | sort | uniq\n",
        "```\n",
        "**Output:**\n",
        "```plaintext\n",
        "This\n",
        "an\n",
        "example\n",
        "file\n",
        "is\n",
        "text\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "T-1NiHb_m4GC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#WEEK-2\n",
        "##Implement a word tokenization using regular expressions"
      ],
      "metadata": {
        "id": "TALvC-EYQuws"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePT0PWKyBzFF",
        "outputId": "cc91898c-214f-42f1-a2a2-e9f2e86bacae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Good', 'muffins', 'cost', '$3.88', 'in', 'New', 'York', '.', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']\n"
          ]
        }
      ],
      "source": [
        "#Week-2\n",
        "#Implement a word tokenization using regular expressions\n",
        "\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "s = \"Good muffins cost $3.88\\nin New York. Please buy me\\ntwo of them.\\n\\nThanks.\"\n",
        "tokenizer = RegexpTokenizer(r'\\w+|\\$[\\d\\.]+|\\S+')\n",
        "print(tokenizer.tokenize(s))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "sentence = \"\"\"At eight o'clock on Thursday morning... Arthur didn't feel very good.\"\"\"\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8KC4xK8DoIc",
        "outputId": "9269d861-e15d-4597-e58c-7b466481beee"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['At', 'eight', \"o'clock\", 'on', 'Thursday', 'morning', '...', 'Arthur', 'did', \"n't\", 'feel', 'very', 'good', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Example 1: Abbreviations like V.C.E, C.S.E, U.S.A\n",
        "text1 = \"This is V.C.E C.S.E I am a Student . I paid a fees of 13.0\"\n",
        "abbreviations = re.findall(r\"(?:[A-Z]\\.)+[A-Z]\", text1)  # Regex to match abbreviations\n",
        "print(\"Abbreviations:\", abbreviations)\n",
        "\n",
        "# Example 2: Splitting text into tokens\n",
        "text2 = \"That U.S.A poster-print costs $12.40... which is 3.45.\"\n",
        "tokens = re.split(r\"\\s+\", text2)  # Split text by whitespace\n",
        "print(\"Tokens:\", tokens)\n",
        "\n",
        "# Example 3: Hyphenated words\n",
        "hyphenated_words = re.findall(r\"\\w+-\\w+\", text2)  # Matches words with hyphens\n",
        "print(\"Hyphenated words:\", hyphenated_words)\n",
        "\n",
        "# Example 4: Alternate way to match hyphenated words\n",
        "hyphenated_words_alt = re.findall(r\"(?:\\w+-\\w+)\", text2)  # Non-capturing group for hyphenated words\n",
        "print(\"Hyphenated words (non-capturing):\", hyphenated_words_alt)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8cH51IyEOFv",
        "outputId": "6ea488e2-4c5f-48da-fbda-8c9d43528483"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Abbreviations: ['V.C.E', 'C.S.E']\n",
            "Tokens: ['That', 'U.S.A', 'poster-print', 'costs', '$12.40...', 'which', 'is', '3.45.']\n",
            "Hyphenated words: ['poster-print']\n",
            "Hyphenated words (non-capturing): ['poster-print']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Sample text\n",
        "text = \"This is V.C.E C.S.E I am a Student. I paid a fees of 13.0\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(text)\n",
        "print(\"Tokenized words:\", tokens)\n",
        "\n",
        "# Remove stopwords\n",
        "li = []\n",
        "for w in tokens:\n",
        "    if w.lower() not in stopwords.words('english'):  # Convert to lowercase for comparison\n",
        "        li.append(w)\n",
        "\n",
        "print(\"Words after removing stopwords:\", li)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iApZgXFQFWMG",
        "outputId": "79c0a652-acee-463b-c6fb-a15b768e79a6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized words: ['This', 'is', 'V.C.E', 'C.S.E', 'I', 'am', 'a', 'Student', '.', 'I', 'paid', 'a', 'fees', 'of', '13.0']\n",
            "Words after removing stopwords: ['V.C.E', 'C.S.E', 'Student', '.', 'paid', 'fees', '13.0']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WEEK 3\n",
        "##Implement Minimum Edit Distance (MED) algorithm for spelling correction"
      ],
      "metadata": {
        "id": "QAh18ZkXQ3o7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Minimum Edit Distance Algorithm\n",
        "source = 'kitten'  # Source string\n",
        "target = 'sitting'  # Target string\n",
        "\n",
        "m = len(source)\n",
        "n = len(target)\n",
        "\n",
        "# Initialize a 2D DP array with dimensions (n+1) x (m+1)\n",
        "dp = [[0 for _ in range(m + 1)] for _ in range(n + 1)]\n",
        "\n",
        "# Fill the base cases for the DP table\n",
        "for i in range(m + 1):\n",
        "    dp[0][i] = i  # Cost of deleting characters from source\n",
        "for j in range(n + 1):\n",
        "    dp[j][0] = j  # Cost of inserting characters into source\n",
        "\n",
        "# Compute the DP table\n",
        "for i in range(1, n + 1):\n",
        "    for j in range(1, m + 1):\n",
        "        if source[j - 1] == target[i - 1]:\n",
        "            cost = 0  # No cost if characters match\n",
        "        else:\n",
        "            cost = 1  # Substitution cost if characters don't match\n",
        "\n",
        "        # Calculate the minimum cost among insertion, deletion, and substitution\n",
        "        dp[i][j] = min(\n",
        "            dp[i - 1][j] + 1,      # Cost of deletion\n",
        "            dp[i][j - 1] + 1,      # Cost of insertion\n",
        "            dp[i - 1][j - 1] + cost  # Cost of substitution\n",
        "        )\n",
        "\n",
        "# Display the DP table (optional)\n",
        "print(\"DP Table:\")\n",
        "for row in dp:\n",
        "    print(row)\n",
        "\n",
        "# Output the minimum edit distance\n",
        "print(\"Edit Distance:\", dp[n][m])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzPdQQ8QRCDD",
        "outputId": "450a2f16-b6c7-499e-f82d-0322013c1b86"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DP Table:\n",
            "[0, 1, 2, 3, 4, 5, 6]\n",
            "[1, 1, 2, 3, 4, 5, 6]\n",
            "[2, 2, 1, 2, 3, 4, 5]\n",
            "[3, 3, 2, 1, 2, 3, 4]\n",
            "[4, 4, 3, 2, 1, 2, 3]\n",
            "[5, 5, 4, 3, 2, 2, 3]\n",
            "[6, 6, 5, 4, 3, 3, 2]\n",
            "[7, 7, 6, 5, 4, 4, 3]\n",
            "Edit Distance: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "def find_minimum_edit_distance(word1, word2):\n",
        "    # Use NLTK's edit_distance function\n",
        "    distance = nltk.edit_distance(word1, word2)\n",
        "    return distance\n",
        "\n",
        "# Example usage\n",
        "word1 = \"kitten\"\n",
        "word2 = \"sitting\"\n",
        "\n",
        "min_edit_distance = find_minimum_edit_distance(word1, word2)\n",
        "print(f\"The minimum edit distance between '{word1}' and '{word2}' is: {min_edit_distance}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guj-m78KT7Wq",
        "outputId": "472af6ef-2aea-4555-d1e1-4511a829fd83"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The minimum edit distance between 'kitten' and 'sitting' is: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WEEK 4\n",
        "##Implement n-gram language model"
      ],
      "metadata": {
        "id": "MrzdYdAlQ4MY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "from nltk.lm.preprocessing import pad_both_ends, padded_everygram_pipeline\n",
        "from nltk.util import bigrams\n",
        "from nltk.lm import MLE\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('brown')\n",
        "\n",
        "# Load the Brown corpus (news category sentences)\n",
        "corpus = brown.sents(categories=\"news\")\n",
        "\n",
        "# Example test sentence\n",
        "test_sentence = ['There', \"wasn't\", 'a', 'bit', 'of', 'trouble', 'in', 'Texas']\n",
        "\n",
        "# Convert test sentence to bigrams with padding\n",
        "test_sentence_bigrams = list(bigrams(pad_both_ends(test_sentence, n=2)))\n",
        "print(\"Test sentence bigrams (with padding):\", test_sentence_bigrams)\n",
        "\n",
        "# Prepare the training data using padded everygram pipeline\n",
        "n = 2  # Bigram model\n",
        "train_data, vocab = padded_everygram_pipeline(n, corpus)\n",
        "\n",
        "# Create an MLE (Maximum Likelihood Estimation) language model\n",
        "lm = MLE(n)\n",
        "lm.fit(train_data, vocab)\n",
        "\n",
        "# Vocabulary size\n",
        "print('Number of words in vocabulary is:', len(lm.vocab))\n",
        "\n",
        "# Language model counts (debugging information)\n",
        "print(\"Language model counts (partial view):\", lm.counts)\n",
        "\n",
        "# Calculate the probability of the test sentence\n",
        "prob = 1.0\n",
        "for t in test_sentence_bigrams:\n",
        "    score = lm.score(t[1], [t[0]])  # P(word | context)\n",
        "    print(f\"Probability of '{t[1]}' given '{t[0]}': {score}\")\n",
        "    prob *= score\n",
        "\n",
        "print(\"Probability of the entire test sentence:\", prob)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fWIlhhSUKAd",
        "outputId": "99443d72-d8bd-4769-dc68-029f67e71f40"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test sentence bigrams (with padding): [('<s>', 'There'), ('There', \"wasn't\"), (\"wasn't\", 'a'), ('a', 'bit'), ('bit', 'of'), ('of', 'trouble'), ('trouble', 'in'), ('in', 'Texas'), ('Texas', '</s>')]\n",
            "Number of words in vocabulary is: 14397\n",
            "Language model counts (partial view): <NgramCounter with 2 ngram orders and 214977 ngrams>\n",
            "Probability of 'There' given '<s>': 0.011464417045208739\n",
            "Probability of 'wasn't' given 'There': 0.017241379310344827\n",
            "Probability of 'a' given 'wasn't': 0.3333333333333333\n",
            "Probability of 'bit' given 'a': 0.002508780732563974\n",
            "Probability of 'of' given 'bit': 0.2857142857142857\n",
            "Probability of 'trouble' given 'of': 0.001053001053001053\n",
            "Probability of 'in' given 'trouble': 0.375\n",
            "Probability of 'Texas' given 'in': 0.001584786053882726\n",
            "Probability of '</s>' given 'Texas': 0.125\n",
            "Probability of the entire test sentence: 3.6943506664397765e-15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WEEK 5\n",
        "##Implement Naïve Bayes classification for sentiment analysis"
      ],
      "metadata": {
        "id": "Zm04qY3ZQ4nv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import movie_reviews\n",
        "import random\n",
        "\n",
        "# Download movie_reviews corpus if not already downloaded\n",
        "nltk.download('movie_reviews')\n",
        "\n",
        "# Prepare the dataset\n",
        "documents = [(list(movie_reviews.words(fileid)), category)\n",
        "             for category in movie_reviews.categories()\n",
        "             for fileid in movie_reviews.fileids(category)]\n",
        "random.shuffle(documents)\n",
        "\n",
        "# Create a list of the most frequent words in the corpus\n",
        "all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())\n",
        "word_features = list(all_words)[:2000]  # Top 2000 frequent words\n",
        "\n",
        "# Function to extract features from a document\n",
        "def document_features(document):\n",
        "    document_words = set(document)\n",
        "    features = {}\n",
        "    for word in word_features:\n",
        "        features['contains({})'.format(word)] = (word in document_words)\n",
        "    return features\n",
        "\n",
        "# Create feature sets\n",
        "featuresets = [(document_features(d), c) for (d, c) in documents]\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "train_set, test_set = featuresets[100:], featuresets[:100]\n",
        "\n",
        "# Train a Naïve Bayes Classifier\n",
        "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
        "\n",
        "# Evaluate the classifier on the test set\n",
        "confusion_matrix = {\"tp\": 0, \"tn\": 0, \"fp\": 0, \"fn\": 0}\n",
        "\n",
        "for test_case in test_set:\n",
        "    predicted = classifier.classify(test_case[0])\n",
        "    actual = test_case[1]\n",
        "\n",
        "    if predicted == 'pos' and actual == 'pos':\n",
        "        confusion_matrix['tp'] += 1\n",
        "    elif predicted == 'neg' and actual == 'neg':\n",
        "        confusion_matrix['tn'] += 1\n",
        "    elif predicted == 'pos' and actual == 'neg':\n",
        "        confusion_matrix['fp'] += 1\n",
        "    else:\n",
        "        confusion_matrix['fn'] += 1\n",
        "\n",
        "# Display the results\n",
        "print(\"Confusion Matrix:\")\n",
        "print(\"True Positives (TP):\", confusion_matrix['tp'])\n",
        "print(\"True Negatives (TN):\", confusion_matrix['tn'])\n",
        "print(\"False Positives (FP):\", confusion_matrix['fp'])\n",
        "print(\"False Negatives (FN):\", confusion_matrix['fn'])\n",
        "\n",
        "# Accuracy of the model\n",
        "accuracy = (confusion_matrix['tp'] + confusion_matrix['tn']) / len(test_set)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Display the most informative features\n",
        "print(\"\\nMost Informative Features:\")\n",
        "classifier.show_most_informative_features(10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsZxFlImVPvH",
        "outputId": "4edc621d-e966-499e-dd0b-f901a9c00819"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "True Positives (TP): 38\n",
            "True Negatives (TN): 39\n",
            "False Positives (FP): 8\n",
            "False Negatives (FN): 15\n",
            "Accuracy: 0.77\n",
            "\n",
            "Most Informative Features:\n",
            "Most Informative Features\n",
            "   contains(outstanding) = True              pos : neg    =     10.7 : 1.0\n",
            "         contains(mulan) = True              pos : neg    =      9.1 : 1.0\n",
            "        contains(seagal) = True              neg : pos    =      7.8 : 1.0\n",
            "   contains(wonderfully) = True              pos : neg    =      6.5 : 1.0\n",
            "         contains(damon) = True              pos : neg    =      6.1 : 1.0\n",
            "         contains(flynt) = True              pos : neg    =      5.7 : 1.0\n",
            "        contains(wasted) = True              neg : pos    =      5.6 : 1.0\n",
            "          contains(lame) = True              neg : pos    =      5.4 : 1.0\n",
            "        contains(poorly) = True              neg : pos    =      5.4 : 1.0\n",
            "         contains(awful) = True              neg : pos    =      5.2 : 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WEEK 6\n",
        "##Implement POS tagging using HMM"
      ],
      "metadata": {
        "id": "wO15CHoSQ4_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "from nltk.tag import hmm\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('brown')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load the tagged sentences from the Brown corpus\n",
        "sentences = brown.tagged_sents()\n",
        "\n",
        "# Train a Hidden Markov Model (HMM) POS tagger\n",
        "trainer = hmm.HiddenMarkovModelTrainer()\n",
        "tagger = trainer.train(sentences)\n",
        "\n",
        "# Example input text for POS tagging\n",
        "text = \"This is a sample sentence for POS tagging in Python.\"\n",
        "words = nltk.word_tokenize(text)\n",
        "\n",
        "# Perform POS tagging using the trained HMM tagger\n",
        "tags = tagger.tag(words)\n",
        "\n",
        "# Display the words with their corresponding tags\n",
        "for word, tag in tags:\n",
        "    print(f\"{word}: {tag}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LWrj-1zVxwP",
        "outputId": "3257c099-0f12-4be7-d1bb-c1093a46fda5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/tag/hmm.py:333: RuntimeWarning: overflow encountered in cast\n",
            "  X[i, j] = self._transitions[si].logprob(self._states[j])\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/tag/hmm.py:335: RuntimeWarning: overflow encountered in cast\n",
            "  O[i, k] = self._output_logprob(si, self._symbols[k])\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/tag/hmm.py:331: RuntimeWarning: overflow encountered in cast\n",
            "  P[i] = self._priors.logprob(si)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This: DT\n",
            "is: BEZ\n",
            "a: AT\n",
            "sample: NN\n",
            "sentence: NN\n",
            "for: IN\n",
            "POS: AT\n",
            "tagging: AT\n",
            "in: AT\n",
            "Python: AT\n",
            ".: AT\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nltk/tag/hmm.py:363: RuntimeWarning: overflow encountered in cast\n",
            "  O[i, k] = self._output_logprob(si, self._symbols[k])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "from nltk.tag import hmm\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('brown')\n",
        "\n",
        "# Load Brown corpus tagged sentences (news category)\n",
        "brown_tagged_sentences = brown.tagged_sents(categories='news')\n",
        "\n",
        "# Split data into training and testing sets (90% train, 10% test)\n",
        "size = int(len(brown_tagged_sentences) * 0.9)\n",
        "train_sentences = brown_tagged_sentences[:size]\n",
        "test_sentences = brown_tagged_sentences[:size]\n",
        "\n",
        "# Train an HMM POS tagger\n",
        "trainer = hmm.HiddenMarkovModelTrainer()\n",
        "tagger = trainer.train(train_sentences)\n",
        "\n",
        "# Evaluate the tagger on the test data\n",
        "accuracy = tagger.accuracy(test_sentences)\n",
        "print(f\"Accuracy of HMM POS Tagger: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aATpLpwPV8cX",
        "outputId": "815a07ca-0db6-4562-f11f-794d110b9432"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of HMM POS Tagger: 0.98\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WEEK 7\n",
        "##Implement CKY parsing algorithm"
      ],
      "metadata": {
        "id": "ys7j_OhMQ5Xv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_chart(chart, n):\n",
        "    # Function to print the chart (parse table)\n",
        "    for p in range(n+1):\n",
        "        for q in range(n+1):\n",
        "            print(chart[p][q], end=\"\\t\")\n",
        "        print()\n",
        "\n",
        "def CKY_PARSE(words, grammar):\n",
        "    n = len(words)\n",
        "    print(\"Words:\", words)\n",
        "\n",
        "    # Initialize parse table (chart)\n",
        "    table = [[set() for _ in range(n+1)] for _ in range(n+1)]\n",
        "\n",
        "    # Fill the diagonal (base case) of the chart\n",
        "    for i in range(1, n+1):\n",
        "        word = words[i-1]\n",
        "        for lhs, rhs in grammar:\n",
        "            if rhs == (word,):  # if the word matches the RHS of a grammar rule\n",
        "                table[i-1][i].add(lhs)\n",
        "\n",
        "    # Fill the rest of the chart (dynamic programming part)\n",
        "    for j in range(2, n+1):  # length of the span\n",
        "        for i in range(j-2, -1, -1):  # starting point of the span\n",
        "            for k in range(i+1, j):  # partitioning point\n",
        "                for lhs, rhs in grammar:\n",
        "                    if len(rhs) == 2:  # we are handling binary productions\n",
        "                        if rhs[0] in table[i][k] and rhs[1] in table[k][j]:\n",
        "                            table[i][j].add(lhs)\n",
        "\n",
        "    return table\n",
        "\n",
        "# Sentence and Grammar Setup\n",
        "sentence = \"the dog chased the cat\"\n",
        "words = sentence.split()\n",
        "n = len(words)\n",
        "\n",
        "grammar = [\n",
        "    ('S', ('NP', 'VP')),\n",
        "    ('NP', ('DET', 'NOMINAL')),\n",
        "    ('VP', ('VERB', 'NP')),\n",
        "    ('NOMINAL', ('cat',)),\n",
        "    ('NOMINAL', ('dog',)),\n",
        "    ('VERB', ('chased',)),\n",
        "    ('DET', ('the',))\n",
        "]\n",
        "\n",
        "# Perform CKY parsing\n",
        "chart = CKY_PARSE(words, grammar)\n",
        "\n",
        "# Print the parse table\n",
        "print_chart(chart, n)\n",
        "\n",
        "# Check if the start symbol is in the final cell\n",
        "start_symbol = 'S'\n",
        "if start_symbol in chart[0][n]:\n",
        "    print(\"The sentence is grammatically correct.\")\n",
        "else:\n",
        "    print(\"The sentence is not grammatically correct.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jv1WNlgYTL_",
        "outputId": "77e93c6c-ae83-40db-bd51-054922a2f291"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words: ['the', 'dog', 'chased', 'the', 'cat']\n",
            "set()\t{'DET'}\t{'NP'}\tset()\tset()\t{'S'}\t\n",
            "set()\tset()\t{'NOMINAL'}\tset()\tset()\tset()\t\n",
            "set()\tset()\tset()\t{'VERB'}\tset()\t{'VP'}\t\n",
            "set()\tset()\tset()\tset()\t{'DET'}\t{'NP'}\t\n",
            "set()\tset()\tset()\tset()\tset()\t{'NOMINAL'}\t\n",
            "set()\tset()\tset()\tset()\tset()\tset()\t\n",
            "The sentence is grammatically correct.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WEEK 8\n",
        "##Implement PCKY parsing algorithm"
      ],
      "metadata": {
        "id": "5vm6jiQZYlMz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_chart(chart, n, non_terminals):\n",
        "    for p in range(n + 1):\n",
        "        for q in range(n + 1):\n",
        "            print('[', p, ',', q, ']:', end=\" \")\n",
        "            for nt in non_terminals:\n",
        "                if chart[p][q].get(nt, 0) > 0:\n",
        "                    print('{', nt, ':', chart[p][q][nt], '}', end=\" \")\n",
        "            print()\n",
        "        print()\n",
        "\n",
        "def PCKY_PARSE(words, grammar, non_terminals):\n",
        "    n = len(words)\n",
        "    print(words, end=\"\\n\")\n",
        "\n",
        "    # Initialize parse table\n",
        "    table = [[dict() for _ in range(n + 1)] for _ in range(n + 1)]\n",
        "\n",
        "    # Initialize table cells with zero probabilities\n",
        "    for i in range(n + 1):\n",
        "        for j in range(n + 1):\n",
        "            for nt in non_terminals:\n",
        "                table[i][j][nt] = 0.0\n",
        "\n",
        "    # Fill table cells for the base case\n",
        "    for j in range(1, n + 1):\n",
        "        for lhs, rhs, pr in grammar:\n",
        "            if rhs == (words[j - 1],):  # terminal case\n",
        "                table[j - 1][j][lhs] = pr\n",
        "\n",
        "    # Fill the table for larger spans\n",
        "    for length in range(2, n + 1):  # span length\n",
        "        for i in range(n - length + 1):  # starting point\n",
        "            j = i + length  # ending point\n",
        "            for k in range(i + 1, j):  # partition point\n",
        "                for lhs, rhs, pr in grammar:\n",
        "                    if len(rhs) == 2:  # binary rule\n",
        "                        if (rhs[0] in table[i][k]) and (rhs[1] in table[k][j]):\n",
        "                            prob = pr * table[i][k].get(rhs[0], 0) * table[k][j].get(rhs[1], 0)\n",
        "                            if table[i][j].get(lhs, 0) < prob:\n",
        "                                table[i][j][lhs] = prob\n",
        "\n",
        "    return table\n",
        "\n",
        "# Sentence and Grammar Setup\n",
        "sentence = \"the flight includes a meal\"\n",
        "words = sentence.split()\n",
        "n = len(words)\n",
        "\n",
        "grammar = [\n",
        "    ('S', ('NP', 'VP'), 0.80),\n",
        "    ('NP', ('DET', 'NOMINAL'), 0.30),\n",
        "    ('VP', ('VERB', 'NP'), 0.20),\n",
        "    ('NOMINAL', ('meal',), 0.01),\n",
        "    ('NOMINAL', ('flight',), 0.02),\n",
        "    ('VERB', ('includes',), 0.05),\n",
        "    ('DET', ('the',), 0.40),\n",
        "    ('DET', ('a',), 0.40)\n",
        "]\n",
        "\n",
        "non_terminals = ['S', 'NP', 'VP', 'DET', 'NOMINAL', 'VERB']\n",
        "\n",
        "# Perform PCKY parsing\n",
        "chart = PCKY_PARSE(words, grammar, non_terminals)\n",
        "\n",
        "# Print the parse table (chart)\n",
        "print_chart(chart, n, non_terminals)\n",
        "\n",
        "# Check if the start symbol is in the final cell and output the result\n",
        "start_symbol = 'S'\n",
        "if chart[0][n].get(start_symbol, 0) > 0:\n",
        "    print(\"The sentence is grammatically correct.\")\n",
        "else:\n",
        "    print(\"The sentence is not grammatically correct.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1MceV1PYtQG",
        "outputId": "b2995019-6a74-4f70-a272-e015b3f520e7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'flight', 'includes', 'a', 'meal']\n",
            "[ 0 , 0 ]: \n",
            "[ 0 , 1 ]: { DET : 0.4 } \n",
            "[ 0 , 2 ]: { NP : 0.0024 } \n",
            "[ 0 , 3 ]: \n",
            "[ 0 , 4 ]: \n",
            "[ 0 , 5 ]: { S : 2.3040000000000003e-08 } \n",
            "\n",
            "[ 1 , 0 ]: \n",
            "[ 1 , 1 ]: \n",
            "[ 1 , 2 ]: { NOMINAL : 0.02 } \n",
            "[ 1 , 3 ]: \n",
            "[ 1 , 4 ]: \n",
            "[ 1 , 5 ]: \n",
            "\n",
            "[ 2 , 0 ]: \n",
            "[ 2 , 1 ]: \n",
            "[ 2 , 2 ]: \n",
            "[ 2 , 3 ]: { VERB : 0.05 } \n",
            "[ 2 , 4 ]: \n",
            "[ 2 , 5 ]: { VP : 1.2000000000000002e-05 } \n",
            "\n",
            "[ 3 , 0 ]: \n",
            "[ 3 , 1 ]: \n",
            "[ 3 , 2 ]: \n",
            "[ 3 , 3 ]: \n",
            "[ 3 , 4 ]: { DET : 0.4 } \n",
            "[ 3 , 5 ]: { NP : 0.0012 } \n",
            "\n",
            "[ 4 , 0 ]: \n",
            "[ 4 , 1 ]: \n",
            "[ 4 , 2 ]: \n",
            "[ 4 , 3 ]: \n",
            "[ 4 , 4 ]: \n",
            "[ 4 , 5 ]: { NOMINAL : 0.01 } \n",
            "\n",
            "[ 5 , 0 ]: \n",
            "[ 5 , 1 ]: \n",
            "[ 5 , 2 ]: \n",
            "[ 5 , 3 ]: \n",
            "[ 5 , 4 ]: \n",
            "[ 5 , 5 ]: \n",
            "\n",
            "The sentence is grammatically correct.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WEEK 9\n",
        "##Compute cosine similarity between the words using term document matrix and term-term matrix"
      ],
      "metadata": {
        "id": "tVeYrjqjZu9k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import random\n",
        "import math\n",
        "from nltk.corpus import brown\n",
        "\n",
        "# Function to extract words from a document\n",
        "def extract_words(document):\n",
        "    all_terms_list = brown.words(fileids=document)\n",
        "    only_words_list = [w.lower() for w in all_terms_list if w.isalpha()]\n",
        "    stopwords_list = nltk.corpus.stopwords.words('english')\n",
        "    final_terms_list = [w for w in only_words_list if w not in stopwords_list]\n",
        "    return final_terms_list\n",
        "\n",
        "# Function to calculate the frequency of a word in a document\n",
        "def freq(word, document):\n",
        "    d_terms = extract_words(document)\n",
        "    fdist = nltk.FreqDist(w for w in d_terms)\n",
        "    return fdist[word]\n",
        "\n",
        "# List of documents from the Brown corpus\n",
        "doc_names = ['ca01', 'ca02', 'ca03', 'ca04']\n",
        "\n",
        "# Constructing the vocabulary set (unique words across all documents)\n",
        "vocab = set()\n",
        "for doc in doc_names:\n",
        "    vocab.update(extract_words(doc))\n",
        "vocab_len = len(vocab)\n",
        "print(\"Length of vocabulary:\", vocab_len)\n",
        "\n",
        "# Randomly select two words from the vocabulary\n",
        "word1 = random.choice(list(vocab))\n",
        "word2 = random.choice(list(vocab))\n",
        "print(\"word-1:\", word1)\n",
        "print(\"word-2:\", word2)\n",
        "\n",
        "# Constructing term frequency vectors for the two words\n",
        "word1_vector = [freq(word1, doc) for doc in doc_names]\n",
        "word2_vector = [freq(word2, doc) for doc in doc_names]\n",
        "print(\"word-1 vector:\", word1_vector)\n",
        "print(\"word-2 vector:\", word2_vector)\n",
        "\n",
        "# Compute the cosine similarity between the two word vectors\n",
        "dot_product = sum(word1_vector[i] * word2_vector[i] for i in range(len(doc_names)))\n",
        "\n",
        "sum1 = sum(word1_vector[i] * word1_vector[i] for i in range(len(doc_names)))\n",
        "sum2 = sum(word2_vector[i] * word2_vector[i] for i in range(len(doc_names)))\n",
        "\n",
        "vector1_len = math.sqrt(sum1)\n",
        "vector2_len = math.sqrt(sum2)\n",
        "\n",
        "# Cosine similarity formula\n",
        "cos_theta = dot_product / (vector1_len * vector2_len)\n",
        "print(f\"Cosine similarity between {word1} and {word2}: {cos_theta}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Ph_AUpWZ7Ew",
        "outputId": "0cc32e03-87ba-437f-b412-d50b312a5119"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of vocabulary: 2023\n",
            "word-1: institute\n",
            "word-2: indispensable\n",
            "word-1 vector: [0, 1, 1, 2]\n",
            "word-2 vector: [0, 0, 1, 0]\n",
            "Cosine similarity between institute and indispensable: 0.4082482904638631\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WEEK 10\n",
        "##Compute tf-idf matrix for the given document set"
      ],
      "metadata": {
        "id": "esFe3EBBbxxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import random\n",
        "import math\n",
        "from nltk.corpus import brown\n",
        "\n",
        "# Function to calculate Term Frequency (TF)\n",
        "def term_freq(word, document):\n",
        "    d_terms = extract_words(document)\n",
        "    fdist = nltk.FreqDist(w for w in d_terms)\n",
        "    return math.log10(fdist[word] + 1)  # log-transformation of term frequency\n",
        "\n",
        "# Function to extract words from a document\n",
        "def extract_words(document):\n",
        "    all_terms_list = brown.words(fileids=document)\n",
        "    only_words_list = [w.lower() for w in all_terms_list if w.isalpha()]\n",
        "    stopwords_list = nltk.corpus.stopwords.words('english')\n",
        "    final_terms_list = [w for w in only_words_list if w not in stopwords_list]\n",
        "    return final_terms_list\n",
        "\n",
        "# Function to calculate Inverse Document Frequency (IDF)\n",
        "def idf(word, doc_names):\n",
        "    df = 0\n",
        "    for doc in doc_names:\n",
        "        if word in extract_words(doc):\n",
        "            df += 1\n",
        "    return math.log10(len(doc_names) / (df + 1))  # log-transformation of IDF\n",
        "\n",
        "# List of documents from the Brown corpus\n",
        "doc_names = ['ca01', 'ca02', 'ca03', 'ca04']\n",
        "\n",
        "# Construct vocabulary set (unique words across all documents)\n",
        "vocab = set()\n",
        "for doc in doc_names:\n",
        "    vocab.update(extract_words(doc))\n",
        "vocab_len = len(vocab)\n",
        "print(\"Length of vocabulary:\", vocab_len)\n",
        "\n",
        "# Randomly select two words from the vocabulary\n",
        "word1 = random.choice(list(vocab))\n",
        "word2 = random.choice(list(vocab))\n",
        "print(\"word-1:\", word1)\n",
        "print(\"word-2:\", word2)\n",
        "\n",
        "# Constructing term frequency vectors (TF * IDF) for the two words\n",
        "word1_vector = [term_freq(word1, doc) * idf(word1, doc_names) for doc in doc_names]\n",
        "word2_vector = [term_freq(word2, doc) * idf(word2, doc_names) for doc in doc_names]\n",
        "print(\"word-1 vector:\", word1_vector)\n",
        "print(\"word-2 vector:\", word2_vector)\n",
        "\n",
        "# Computing cosine similarity between word1 and word2\n",
        "dot_product = sum(word1_vector[i] * word2_vector[i] for i in range(len(doc_names)))\n",
        "\n",
        "sum1 = sum(word1_vector[i] * word1_vector[i] for i in range(len(doc_names)))\n",
        "sum2 = sum(word2_vector[i] * word2_vector[i] for i in range(len(doc_names)))\n",
        "\n",
        "vector1_len = math.sqrt(sum1)\n",
        "vector2_len = math.sqrt(sum2)\n",
        "\n",
        "# Cosine similarity formula\n",
        "cos_theta = dot_product / (vector1_len * vector2_len)\n",
        "print(f\"Cosine similarity between {word1} and {word2}: {cos_theta}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGGdG3iPb3KR",
        "outputId": "a128811c-1d2c-4962-f0e9-18f77cefd346"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of vocabulary: 2023\n",
            "word-1: limited\n",
            "word-2: aged\n",
            "word-1 vector: [0.0, 0.03761030733945982, 0.0, 0.05961092677364148]\n",
            "word-2 vector: [0.0, 0.0, 0.2342468675289098, 0.0]\n",
            "Cosine similarity between limited and aged: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WEEK 11\n",
        "##IMPLEMENT LANGUAGE MODEL USING FEEDFORWARD NEURALNETWORK"
      ],
      "metadata": {
        "id": "ojHssCeWb_Ib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "# Toy dataset\n",
        "corpus = [\n",
        "    'This is a simple example',\n",
        "    'Language modeling is interesting',\n",
        "    'Neural networks are powerful',\n",
        "    'Feed-forward networks are common in NLP'\n",
        "]\n",
        "\n",
        "# Tokenize the text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "total_words = len(tokenizer.word_index) + 1  # +1 for padding\n",
        "\n",
        "# Create input sequences and labels\n",
        "input_sequences = []\n",
        "for line in corpus:\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "\n",
        "# Pad sequences to ensure they all have the same length\n",
        "max_sequence_length = max([len(x) for x in input_sequences])\n",
        "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_length, padding='pre')\n",
        "\n",
        "# Split into X (input) and y (target)\n",
        "X, y = input_sequences[:, :-1], input_sequences[:, -1]\n",
        "\n",
        "# Convert y to categorical (one-hot encoding)\n",
        "y = tf.keras.utils.to_categorical(y, num_classes=total_words)\n",
        "\n",
        "# Build the model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(total_words, 50, input_length=max_sequence_length-1),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(100, activation='relu'),\n",
        "    tf.keras.layers.Dense(total_words, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, y, epochs=100, verbose=1)\n",
        "\n",
        "# Generate text using the trained model\n",
        "seed_text = \"Neural networks\"\n",
        "next_words = 5\n",
        "\n",
        "for _ in range(next_words):\n",
        "    # Convert seed text to sequence of integers\n",
        "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen=max_sequence_length-1, padding='pre')\n",
        "\n",
        "    # Predict the next word\n",
        "    predicted = np.argmax(model.predict(token_list), axis=-1)\n",
        "\n",
        "    # Convert the predicted integer back to the word\n",
        "    output_word = \"\"\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == predicted:\n",
        "            output_word = word\n",
        "            break\n",
        "\n",
        "    # Append the predicted word to the seed text\n",
        "    seed_text += \" \" + output_word\n",
        "\n",
        "print(seed_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPfQeRaPcB8g",
        "outputId": "2c511dd9-64b9-4db9-986e-19d3a81b7530"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.0000e+00 - loss: 2.8938\n",
            "Epoch 2/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.0625 - loss: 2.8709\n",
            "Epoch 3/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.1875 - loss: 2.8500\n",
            "Epoch 4/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.3750 - loss: 2.8305\n",
            "Epoch 5/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.5000 - loss: 2.8120\n",
            "Epoch 6/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.5000 - loss: 2.7933\n",
            "Epoch 7/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.5000 - loss: 2.7743\n",
            "Epoch 8/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.5000 - loss: 2.7548\n",
            "Epoch 9/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.5625 - loss: 2.7344\n",
            "Epoch 10/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.5625 - loss: 2.7125\n",
            "Epoch 11/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.5625 - loss: 2.6893\n",
            "Epoch 12/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.5625 - loss: 2.6647\n",
            "Epoch 13/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.6250 - loss: 2.6387\n",
            "Epoch 14/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.6250 - loss: 2.6112\n",
            "Epoch 15/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.6250 - loss: 2.5821\n",
            "Epoch 16/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.6250 - loss: 2.5516\n",
            "Epoch 17/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.6250 - loss: 2.5195\n",
            "Epoch 18/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.6875 - loss: 2.4854\n",
            "Epoch 19/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.6875 - loss: 2.4495\n",
            "Epoch 20/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.6875 - loss: 2.4118\n",
            "Epoch 21/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.6875 - loss: 2.3722\n",
            "Epoch 22/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.6875 - loss: 2.3304\n",
            "Epoch 23/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.6875 - loss: 2.2867\n",
            "Epoch 24/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.6875 - loss: 2.2413\n",
            "Epoch 25/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.6875 - loss: 2.1940\n",
            "Epoch 26/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.6875 - loss: 2.1450\n",
            "Epoch 27/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.6875 - loss: 2.0945\n",
            "Epoch 28/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.6875 - loss: 2.0425\n",
            "Epoch 29/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.6875 - loss: 1.9892\n",
            "Epoch 30/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.6875 - loss: 1.9348\n",
            "Epoch 31/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.7500 - loss: 1.8796\n",
            "Epoch 32/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.7500 - loss: 1.8235\n",
            "Epoch 33/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.6875 - loss: 1.7669\n",
            "Epoch 34/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.6875 - loss: 1.7096\n",
            "Epoch 35/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.6875 - loss: 1.6517\n",
            "Epoch 36/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.6875 - loss: 1.5933\n",
            "Epoch 37/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.7500 - loss: 1.5345\n",
            "Epoch 38/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.7500 - loss: 1.4753\n",
            "Epoch 39/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.7500 - loss: 1.4159\n",
            "Epoch 40/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.7500 - loss: 1.3563\n",
            "Epoch 41/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.8125 - loss: 1.2969\n",
            "Epoch 42/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8125 - loss: 1.2379\n",
            "Epoch 43/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8750 - loss: 1.1791\n",
            "Epoch 44/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8750 - loss: 1.1210\n",
            "Epoch 45/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8750 - loss: 1.0637\n",
            "Epoch 46/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.8750 - loss: 1.0078\n",
            "Epoch 47/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8750 - loss: 0.9533\n",
            "Epoch 48/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8750 - loss: 0.9004\n",
            "Epoch 49/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9375 - loss: 0.8494\n",
            "Epoch 50/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.8006\n",
            "Epoch 51/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.7540\n",
            "Epoch 52/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.7096\n",
            "Epoch 53/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.6671\n",
            "Epoch 54/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 1.0000 - loss: 0.6268\n",
            "Epoch 55/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 0.5885\n",
            "Epoch 56/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 0.5522\n",
            "Epoch 57/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 1.0000 - loss: 0.5179\n",
            "Epoch 58/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 1.0000 - loss: 0.4854\n",
            "Epoch 59/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 1.0000 - loss: 0.4546\n",
            "Epoch 60/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 1.0000 - loss: 0.4256\n",
            "Epoch 61/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.3982\n",
            "Epoch 62/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 0.3726\n",
            "Epoch 63/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 1.0000 - loss: 0.3486\n",
            "Epoch 64/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 1.0000 - loss: 0.3262\n",
            "Epoch 65/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 0.3052\n",
            "Epoch 66/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 0.2856\n",
            "Epoch 67/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 1.0000 - loss: 0.2673\n",
            "Epoch 68/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 1.0000 - loss: 0.2502\n",
            "Epoch 69/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 1.0000 - loss: 0.2342\n",
            "Epoch 70/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 0.2193\n",
            "Epoch 71/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 0.2053\n",
            "Epoch 72/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 1.0000 - loss: 0.1923\n",
            "Epoch 73/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 1.0000 - loss: 0.1801\n",
            "Epoch 74/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 1.0000 - loss: 0.1687\n",
            "Epoch 75/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 1.0000 - loss: 0.1582\n",
            "Epoch 76/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 0.1483\n",
            "Epoch 77/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.1391\n",
            "Epoch 78/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 0.1305\n",
            "Epoch 79/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.1225\n",
            "Epoch 80/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.1151\n",
            "Epoch 81/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 1.0000 - loss: 0.1081\n",
            "Epoch 82/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.1016\n",
            "Epoch 83/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.0955\n",
            "Epoch 84/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.0899\n",
            "Epoch 85/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 0.0846\n",
            "Epoch 86/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.0797\n",
            "Epoch 87/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.0751\n",
            "Epoch 88/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 1.0000 - loss: 0.0708\n",
            "Epoch 89/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 0.0669\n",
            "Epoch 90/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.0632\n",
            "Epoch 91/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 1.0000 - loss: 0.0598\n",
            "Epoch 92/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.0567\n",
            "Epoch 93/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.0537\n",
            "Epoch 94/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 1.0000 - loss: 0.0510\n",
            "Epoch 95/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 1.0000 - loss: 0.0484\n",
            "Epoch 96/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 1.0000 - loss: 0.0461\n",
            "Epoch 97/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 1.0000 - loss: 0.0439\n",
            "Epoch 98/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 1.0000 - loss: 0.0418\n",
            "Epoch 99/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 1.0000 - loss: 0.0399\n",
            "Epoch 100/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 0.0381\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "Neural networks are powerful in nlp a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WEEK-12\n",
        "##IMPLEMENT LANGUAGE MODEL USING RNN"
      ],
      "metadata": {
        "id": "AtPeqvsmcfQg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "# Toy dataset\n",
        "corpus = [\n",
        "    'This is a simple example',\n",
        "    'Language modeling is interesting',\n",
        "    'Neural networks are powerful',\n",
        "    'Recurrent neural networks capture sequences well'\n",
        "]\n",
        "\n",
        "# Tokenize the text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "total_words = len(tokenizer.word_index) + 1  # +1 for padding\n",
        "\n",
        "# Create input sequences and labels\n",
        "input_sequences = []\n",
        "for line in corpus:\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "\n",
        "# Pad sequences to ensure they all have the same length\n",
        "max_sequence_length = max([len(x) for x in input_sequences])\n",
        "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_length, padding='pre')\n",
        "\n",
        "# Split into X (input) and y (target)\n",
        "X, y = input_sequences[:, :-1], input_sequences[:, -1]\n",
        "\n",
        "# Convert y to categorical (one-hot encoding)\n",
        "y = tf.keras.utils.to_categorical(y, num_classes=total_words)\n",
        "\n",
        "# Build the model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(total_words, 50, input_length=max_sequence_length-1),\n",
        "    tf.keras.layers.LSTM(100),\n",
        "    tf.keras.layers.Dense(total_words, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, y, epochs=100, verbose=1)\n",
        "\n",
        "# Generate text using the trained model\n",
        "seed_text = \"Recurrent neural networks\"\n",
        "next_words = 5\n",
        "\n",
        "for _ in range(next_words):\n",
        "    # Convert seed text to sequence of integers\n",
        "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen=max_sequence_length-1, padding='pre')\n",
        "\n",
        "    # Predict the next word\n",
        "    predicted = np.argmax(model.predict(token_list), axis=-1)\n",
        "\n",
        "    # Convert the predicted integer back to the word\n",
        "    output_word = \"\"\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == predicted:\n",
        "            output_word = word\n",
        "            break\n",
        "\n",
        "    # Append the predicted word to the seed text\n",
        "    seed_text += \" \" + output_word\n",
        "\n",
        "print(seed_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tesw_wUdcibx",
        "outputId": "f0ea0828-e384-40f1-9407-b6d343372764"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.0000e+00 - loss: 2.8330\n",
            "Epoch 2/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.1333 - loss: 2.8268\n",
            "Epoch 3/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.2000 - loss: 2.8206\n",
            "Epoch 4/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.2000 - loss: 2.8144\n",
            "Epoch 5/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.2000 - loss: 2.8080\n",
            "Epoch 6/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.2000 - loss: 2.8013\n",
            "Epoch 7/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.2000 - loss: 2.7942\n",
            "Epoch 8/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.2000 - loss: 2.7866\n",
            "Epoch 9/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.2000 - loss: 2.7784\n",
            "Epoch 10/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.2000 - loss: 2.7694\n",
            "Epoch 11/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.2000 - loss: 2.7596\n",
            "Epoch 12/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.2000 - loss: 2.7487\n",
            "Epoch 13/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.2000 - loss: 2.7367\n",
            "Epoch 14/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.2000 - loss: 2.7231\n",
            "Epoch 15/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.2000 - loss: 2.7080\n",
            "Epoch 16/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.2000 - loss: 2.6909\n",
            "Epoch 17/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.2000 - loss: 2.6717\n",
            "Epoch 18/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.2000 - loss: 2.6501\n",
            "Epoch 19/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.2000 - loss: 2.6260\n",
            "Epoch 20/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.2000 - loss: 2.5992\n",
            "Epoch 21/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.2000 - loss: 2.5698\n",
            "Epoch 22/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.1333 - loss: 2.5385\n",
            "Epoch 23/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.1333 - loss: 2.5064\n",
            "Epoch 24/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.1333 - loss: 2.4753\n",
            "Epoch 25/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.1333 - loss: 2.4483\n",
            "Epoch 26/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.1333 - loss: 2.4277\n",
            "Epoch 27/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.1333 - loss: 2.4129\n",
            "Epoch 28/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.1333 - loss: 2.3992\n",
            "Epoch 29/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.2000 - loss: 2.3818\n",
            "Epoch 30/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.2000 - loss: 2.3584\n",
            "Epoch 31/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.2667 - loss: 2.3302\n",
            "Epoch 32/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.2667 - loss: 2.2993\n",
            "Epoch 33/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.2667 - loss: 2.2679\n",
            "Epoch 34/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.2667 - loss: 2.2373\n",
            "Epoch 35/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.2667 - loss: 2.2075\n",
            "Epoch 36/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.2667 - loss: 2.1778\n",
            "Epoch 37/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.3333 - loss: 2.1473\n",
            "Epoch 38/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.3333 - loss: 2.1148\n",
            "Epoch 39/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.3333 - loss: 2.0796\n",
            "Epoch 40/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.3333 - loss: 2.0411\n",
            "Epoch 41/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.4000 - loss: 1.9990\n",
            "Epoch 42/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.4000 - loss: 1.9534\n",
            "Epoch 43/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.4000 - loss: 1.9046\n",
            "Epoch 44/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.4000 - loss: 1.8532\n",
            "Epoch 45/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.4000 - loss: 1.7997\n",
            "Epoch 46/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.4000 - loss: 1.7446\n",
            "Epoch 47/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.4000 - loss: 1.6884\n",
            "Epoch 48/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.4000 - loss: 1.6313\n",
            "Epoch 49/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.5333 - loss: 1.5736\n",
            "Epoch 50/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.5333 - loss: 1.5154\n",
            "Epoch 51/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.6000 - loss: 1.4572\n",
            "Epoch 52/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.6000 - loss: 1.4001\n",
            "Epoch 53/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6667 - loss: 1.3449\n",
            "Epoch 54/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7333 - loss: 1.2914\n",
            "Epoch 55/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.7333 - loss: 1.2382\n",
            "Epoch 56/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.7333 - loss: 1.1855\n",
            "Epoch 57/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.7333 - loss: 1.1346\n",
            "Epoch 58/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.8000 - loss: 1.0855\n",
            "Epoch 59/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.8000 - loss: 1.0378\n",
            "Epoch 60/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8000 - loss: 0.9927\n",
            "Epoch 61/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8000 - loss: 0.9513\n",
            "Epoch 62/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8000 - loss: 0.9121\n",
            "Epoch 63/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8000 - loss: 0.8744\n",
            "Epoch 64/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.8000 - loss: 0.8395\n",
            "Epoch 65/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8000 - loss: 0.8068\n",
            "Epoch 66/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.8000 - loss: 0.7754\n",
            "Epoch 67/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.8000 - loss: 0.7443\n",
            "Epoch 68/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8667 - loss: 0.7138\n",
            "Epoch 69/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.8667 - loss: 0.6843\n",
            "Epoch 70/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8667 - loss: 0.6562\n",
            "Epoch 71/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.8667 - loss: 0.6296\n",
            "Epoch 72/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8667 - loss: 0.6035\n",
            "Epoch 73/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.8667 - loss: 0.5783\n",
            "Epoch 74/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.8667 - loss: 0.5543\n",
            "Epoch 75/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.8667 - loss: 0.5314\n",
            "Epoch 76/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.8667 - loss: 0.5100\n",
            "Epoch 77/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.9333 - loss: 0.4896\n",
            "Epoch 78/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.9333 - loss: 0.4707\n",
            "Epoch 79/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.9333 - loss: 0.4529\n",
            "Epoch 80/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9333 - loss: 0.4358\n",
            "Epoch 81/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 1.0000 - loss: 0.4194\n",
            "Epoch 82/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 1.0000 - loss: 0.4035\n",
            "Epoch 83/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 1.0000 - loss: 0.3883\n",
            "Epoch 84/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 1.0000 - loss: 0.3736\n",
            "Epoch 85/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.3594\n",
            "Epoch 86/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 1.0000 - loss: 0.3459\n",
            "Epoch 87/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 0.3329\n",
            "Epoch 88/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 1.0000 - loss: 0.3205\n",
            "Epoch 89/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 0.3088\n",
            "Epoch 90/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.2975\n",
            "Epoch 91/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 1.0000 - loss: 0.2868\n",
            "Epoch 92/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 1.0000 - loss: 0.2765\n",
            "Epoch 93/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 1.0000 - loss: 0.2666\n",
            "Epoch 94/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 1.0000 - loss: 0.2571\n",
            "Epoch 95/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 1.0000 - loss: 0.2481\n",
            "Epoch 96/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 1.0000 - loss: 0.2394\n",
            "Epoch 97/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 0.2312\n",
            "Epoch 98/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 0.2233\n",
            "Epoch 99/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.2158\n",
            "Epoch 100/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.2086\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "Recurrent neural networks capture sequences well well well\n"
          ]
        }
      ]
    }
  ]
}